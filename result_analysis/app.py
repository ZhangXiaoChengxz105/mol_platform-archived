import streamlit as st
import yaml
import os
import sys
import subprocess
import pathlib
import pandas as pd
import re
import json
from datetime import datetime
from process import process
try:
    project_root = pathlib.Path(__file__).resolve().parents[1]
except NameError:
    project_root = pathlib.Path(os.getcwd()).resolve().parents[0]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
from models.check_utils import get_datasets_measure_names,CheckUtils
from streamlit_option_menu import option_menu

def set_streamlit_upload_limit(limit_mb=2048):
    config_dir = os.path.expanduser("~/.streamlit")
    os.makedirs(config_dir, exist_ok=True)
    config_path = os.path.join(config_dir, "config.toml")

    with open(config_path, "w") as f:
        f.write(f"[server]\nmaxUploadSize = {limit_mb}\n")

set_streamlit_upload_limit(2048)

st.set_page_config(layout="wide")
st.title("åˆ†å­æ€§è´¨é¢„æµ‹é›†æˆå¹³å°")
st.markdown("æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨åŠ è½½æ•°æ®é›†ï¼Œä»…åœ¨éœ€è¦æ—¶æ˜¾ç¤ºé¢å¤–å‚æ•°ï¼Œæœ€ç»ˆä¿å­˜ä¸ºé…ç½®æ–‡ä»¶å¹¶å¯ä¾›æ¨¡å‹è¿è¡Œã€‚")

# ----------- é…ç½®è·¯å¾„ -----------
MODEL_PATH =os.path.join(project_root,'models')
CONFIG_PATH = os.path.join(project_root,'result_analysis','config_run.yaml')
# MODEL_MAP_PATH = os.path.join(project_root,'models','model_datasets.yaml')
RUN_SCRIPT_PATH = os.path.join(project_root,'result_analysis','run_all.py')
HISTORY_PATH = os.path.join(project_root, 'results', 'results','run_history,json')
MODEL_DATASET_PATH = os.path.join(MODEL_PATH,'models.yaml')




# ----------- åŠ è½½ config.yaml -----------
@st.cache_data
def load_config(path=CONFIG_PATH):
    if not os.path.exists(path):
        return {
            "model": "fp",
            "name": "BBBP",
            "eval": True,
            "target_list": "all",
            "smiles_list": "random200",
            "output": "results",
            "plotpath": "plots",
            "plotprevisousruns": False
        }
    with open(path, "r") as f:
        return yaml.safe_load(f)

def get_all_model_types():
    with open(MODEL_DATASET_PATH,'r') as f:
        config = yaml.safe_load(f)
        return list(config.keys())

def get_models_and_data(top_key):  # top_key æ˜¯ 'moleculenet'
    with open(MODEL_DATASET_PATH, 'r') as f:
        config = yaml.safe_load(f)

    top_config = config.get(top_key, {})
    # æå–æ‰€æœ‰æ¨¡å‹åç»„åˆï¼Œå¦‚ FP_NN, GNN_GIN ç­‰
    models_config = top_config.get('models', {})
    model_names = []
    for model_type, sub_models in models_config.items():
        for sub_model in sub_models:
            model_names.append(f"{model_type}_{sub_model}")
    DATACONFIG_PATH = os.path.join(project_root,'dataset','data',top_key,'dataset.yaml')
    with open(DATACONFIG_PATH, 'r',encoding='utf-8') as g:
        config = yaml.safe_load(g)
    all_datasets = config.get('dataset_names',[])

    return model_names, all_datasets
        
def get_data_type(top_key):
    DATACONFIG_PATH = os.path.join(project_root,'dataset','data',top_key,'dataset.yaml')
    with open(DATACONFIG_PATH, 'r',encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return (config.get('data_type',''))

def display_csv_tables(csv_dir):
    csv_files = [f for f in os.listdir(csv_dir) if f.endswith(".csv")]
    for csv_file in sorted(csv_files):
        csv_path = os.path.join(csv_dir, csv_file)
        with st.expander(f"ğŸ“„ {csv_file}"):
            try:
                df = pd.read_csv(csv_path)
                st.dataframe(df, use_container_width=True)
            except Exception as e:
                st.warning(f"{csv_file} åŠ è½½å¤±è´¥: {e}")
                
def display_images_recursively(base_dir):
    for root, dirs, files in os.walk(base_dir):
        image_files = [f for f in files if f.lower().endswith((".png", ".jpg", ".jpeg"))]
        if image_files:
            rel_path = os.path.relpath(root, base_dir)
            with st.expander(f"ğŸ“‚ {rel_path}"):
                cols = st.columns(2)  # æ¯è¡Œä¸¤åˆ—
                for idx, image in enumerate(sorted(image_files)):
                    image_path = os.path.join(root, image)
                    col = cols[idx % 2]  # äº¤æ›¿å†™å…¥ä¸¤ä¸ªåˆ—
                    with col:
                        st.image(image_path, caption=image, use_container_width="always")

                
def get_latest_run_folder(base="results"):
    run_dirs = [d for d in os.listdir(base) if os.path.isdir(os.path.join(base, d)) and re.match(r"run\d+", d)]
    run_numbers = [int(re.findall(r"run(\d+)", d)[0]) for d in run_dirs]
    if run_numbers:
        latest_run = f"run{max(run_numbers)}"
        return latest_run,os.path.join(base, latest_run)
    return None

def get_submodel(model_type, model):
    with open(MODEL_DATASET_PATH, 'r') as f:
        data = yaml.safe_load(f)
    
    try:
        return list(data[model_type]['models'][model].keys())
    except (KeyError, AttributeError):
        return []

        

def show_file_selector(label, file_path, is_markdown=False, height=500):
    """æ˜¾ç¤ºå¤é€‰æ¡†ï¼Œå‹¾é€‰åå±•ç¤ºå¸¦å›ºå®šé«˜åº¦æ»šåŠ¨æ¡çš„æ–‡ä»¶å†…å®¹"""
    if not os.path.exists(file_path):
        st.write(f"{label} æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return

    show_content = st.checkbox(f"æ˜¾ç¤º {label}", key=f"show_{label}")

    if show_content:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        if is_markdown:
            st.markdown(content)
        else:
            # st.code æ”¯æŒè®¾ç½® heightï¼Œæ˜¾ç¤ºå¸¦æ»šåŠ¨æ¡çš„ä»£ç åŒºåŸŸ
            st.code(content, language="python", line_numbers=True, height=height)


# ----------- ä¿å­˜ config.yaml -----------
def save_config(config, path=CONFIG_PATH):
    with open(path, "w") as f:
        yaml.safe_dump(config, f, allow_unicode=True)
        
def list_to_csv_fields(config_dict, fields):
    for field in fields:
        if isinstance(config_dict.get(field), list):
            config_dict[field] = ",".join(str(x) for x in config_dict[field])
    return config_dict


def get_datasets_for_model(model_list, model_map):
    """
    ä»æ¨¡å‹åˆ—è¡¨ä¸­æå–æ‰€æœ‰æ¨¡å‹æ”¯æŒçš„æ•°æ®é›†ï¼Œå¹¶è¿”å›å®ƒä»¬çš„äº¤é›†ã€‚

    å‚æ•°ï¼š
    - model_list (List[str]): æ¨¡å‹åç§°åˆ—è¡¨ï¼Œå¦‚ ['FP NN', 'GNN GCN']
    - model_map (Dict[str, Dict]): ä» model_datasets.yaml åŠ è½½çš„æ¨¡å‹æ˜ å°„

    è¿”å›ï¼š
    - List[str]: æ‰€æœ‰æ¨¡å‹å…±åŒæ”¯æŒçš„æ•°æ®é›†åç§°åˆ—è¡¨
    """
    all_dataset_sets = []

    for model in model_list:
        try:
            model_name= model.split("_")[0]
            model_type = model.split("_")[1]
        except ValueError:
            continue  # å¿½ç•¥æ ¼å¼é”™è¯¯çš„æ¡ç›®

        datasets = model_map.get(model_name, {}).get(model_type)
        if datasets:
            all_dataset_sets.append(set(datasets))

    if not all_dataset_sets:
        return []

    common_datasets = set.intersection(*all_dataset_sets)
    return sorted(list(common_datasets))

def process(dataset_type,zip):
    return



# ----------- åˆå§‹åŒ– session_state -----------
if "selected_model_field" not in st.session_state:
    st.session_state["selected_model_field"] = None
if "selected_models" not in st.session_state:
    st.session_state["selected_models"] = []
if "selected_datasets" not in st.session_state:
    st.session_state["selected_datasets"] = []
if "eval" not in st.session_state:
    st.session_state["eval"] = True
if "smiles_list" not in st.session_state:
    st.session_state["smiles_list"] = "random200"
if "smiles_input_mode" not in st.session_state:
    st.session_state["smiles_input_mode"] = "auto_eval"  # å¯é€‰: auto_eval, file_upload, manual_input
if "smiles_text_input" not in st.session_state:
    st.session_state["smiles_text_input"] = ""
if "smiles_file" not in st.session_state:
    st.session_state["smiles_file"] = None
if "smiles_eval_mode" not in st.session_state:
    st.session_state["smiles_eval_mode"] = "random"
if "smiles_eval_num" not in st.session_state:
    st.session_state["smiles_eval_num"] = 200





# é¡¶éƒ¨æŒ‰é’®
if "final_model_type" not in st.session_state:
    st.session_state["final_model_type"] = ""
if "uploaded_model_zip" not in st.session_state:
    st.session_state["uploaded_model_zip"] = None
if "uploaded_model_config" not in st.session_state:
    st.session_state["uploaded_model_config"] = None
if "uploaded_data_zip" not in st.session_state:
    st.session_state["uploaded_data_zip"] = None
if "uploaded_data_config" not in st.session_state:
    st.session_state["uploaded_data_config"] = None
if "show_model_input" not in st.session_state:
    st.session_state["show_model_input"] = False

# ----------- å±•å¼€æŒ‰é’® -----------
col1, col2 = st.columns([10, 1])
with col2:
    if st.button("â• æ·»åŠ æ¨¡å‹ç±»å‹"):
        st.session_state["show_model_input"] = not st.session_state["show_model_input"]

# ----------- å±•å¼€åŒºåŸŸ -----------
if st.session_state["show_model_input"]:
    st.markdown("#### ğŸ”§ è‡ªå®šä¹‰æ¨¡å‹ç±»å‹ä¸æ¨¡å‹åŒ…ä¸Šä¼ ")

    try:
        all_model_types = get_all_model_types()
    except Exception as e:
        st.warning(f"åŠ è½½æ¨¡å‹ç±»å‹å¤±è´¥ï¼š{e}")
        all_model_types = []

    # ä¿®æ”¹åçš„é€‰æ‹©æ§ä»¶
    model_type_options = ["è‡ªå®šä¹‰è¾“å…¥"] + all_model_types
    current_index = model_type_options.index(
        st.session_state["final_model_type"] 
        if st.session_state["final_model_type"] in model_type_options 
        else "è‡ªå®šä¹‰è¾“å…¥"
    )

    # ä¸»é€‰æ‹©æ¡† - ç›´æ¥ç»‘å®šåˆ° session_state
    selected_option = st.selectbox(
        "ä»å·²æœ‰æ¨¡å‹ç±»å‹ä¸­é€‰æ‹©æˆ–ç›´æ¥è¾“å…¥æ–°ç±»å‹ï¼š",
        options=model_type_options,
        index=current_index,
        key="model_type_select"  # ç›´æ¥ä½¿ç”¨keyç»‘å®š
    )

    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºè‡ªå®šä¹‰è¾“å…¥æ¡†æˆ–æ¨¡å‹ä¿¡æ¯
    if st.session_state.model_type_select == "è‡ªå®šä¹‰è¾“å…¥":
        st.text_input(
            "è¯·è¾“å…¥æ–°çš„æ¨¡å‹ç±»å‹",
            value=st.session_state.final_model_type,
            key="custom_model_input"  # ç›´æ¥ä½¿ç”¨keyç»‘å®š
        )
        # ç«‹å³æ›´æ–°final_model_type
        st.session_state.final_model_type = st.session_state.custom_model_input
    else:
        st.session_state.final_model_type = st.session_state.model_type_select
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ï¼ˆä¿æŒä¸å˜ï¼‰
        datatype = get_data_type(st.session_state.final_model_type)
        st.markdown(f"**ğŸ§¬ æ¨¡å‹è¾“å…¥æ ¼å¼ï¼š** `{datatype}`")
        models_list, datasets_list = get_models_and_data(st.session_state.final_model_type)
        
        if models_list:
            with st.expander("ğŸ“¦ å·²æœ‰æ¨¡å‹åˆ—è¡¨ (models_list)"):
                st.markdown("\n".join(f"- {item}" for item in models_list))
        if datasets_list:
            with st.expander("ğŸ—‚ï¸ å·²æœ‰æ•°æ®é›†åˆ—è¡¨ (datasets_list)"):
                st.markdown("\n".join(f"- {item}" for item in datasets_list))
    final_model_type = st.session_state.final_model_type

    # ----------- ä¸Šä¼ æ–‡ä»¶åŒºåŸŸ -----------
    uploaded_zip = st.file_uploader("ğŸ“¦ ä¸Šä¼ æ¨¡å‹æ–‡ä»¶åŒ…ï¼ˆmodel.zipï¼‰", type=["zip"])
    if uploaded_zip:
        st.session_state["uploaded_model_zip"] = uploaded_zip
        st.success(f"âœ… ä¸Šä¼ æ¨¡å‹åŒ…ï¼š{uploaded_zip.name}")

    uploaded_model_config = st.file_uploader("ğŸ“„ ä¸Šä¼ æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆmodel_config.yamlï¼‰", type=["yaml"])
    if uploaded_model_config:
        st.session_state["uploaded_model_config"] = uploaded_model_config
        st.success(f"âœ… ä¸Šä¼ æ¨¡å‹é…ç½®ï¼š{uploaded_model_config.name}")
        
    uploaded_data_config = st.file_uploader("ğŸ“„ ä¸Šä¼ æ•°æ®é…ç½®æ–‡ä»¶ï¼ˆdata_config.yamlï¼‰", type=["yaml"])
    if uploaded_data_config:
        st.session_state["uploaded_data_config"] = uploaded_data_config
        st.success(f"âœ… ä¸Šä¼ æ•°æ®é…ç½®ï¼š{uploaded_data_config.name}")

    uploaded_data_zip = st.file_uploader("ğŸ—‚ï¸ ä¸Šä¼ æ•°æ®æ–‡ä»¶åŒ…ï¼ˆdata.zipï¼‰", type=["zip"])
    if uploaded_data_zip:
        st.session_state["uploaded_data_zip"] = uploaded_data_zip
        st.success(f"âœ… ä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼š{uploaded_data_zip.name}")

    # ----------- æ˜¾ç¤ºç”¨æˆ·è¾“å…¥çŠ¶æ€ -----------
    if final_model_type:
        st.success(f"ğŸ¯ é€‰æ‹©/è¾“å…¥çš„æ¨¡å‹ç±»å‹ï¼š`{final_model_type}`")
    if st.button("ğŸš€ æäº¤å¹¶å¤„ç†æ¨¡å‹ç±»å‹"):
        # è·å–ä¸Šä¼ çš„æ–‡ä»¶
        model_zip = st.session_state.get("uploaded_model_zip")
        model_config = st.session_state.get("uploaded_model_config")
        data_zip = st.session_state.get("uploaded_data_zip")
        data_config = st.session_state.get("uploaded_data_config")

        # æ£€æŸ¥æ¨¡å‹ç»„æ˜¯å¦å®Œæ•´
        model_ready = (model_zip is not None) and (model_config is not None)
        # æ£€æŸ¥æ•°æ®ç»„æ˜¯å¦å®Œæ•´
        data_ready = (data_zip is not None) and (data_config is not None)

        # æƒ…å†µ1ï¼šæ¨¡å‹ç»„å®Œæ•´ï¼Œdata_zip å¯ä»¥ç¼ºå¤±ï¼ˆä½† data_config å¿…é¡»ä¼ ï¼‰
        condition1 = model_ready and (data_config is not None)
        # æƒ…å†µ2ï¼šæ•°æ®ç»„å®Œæ•´ï¼Œæ¨¡å‹ç»„å¯ä»¥å®Œå…¨ç¼ºå¤±
        condition2 = data_ready and (not model_ready)
        condition3 = model_ready and data_ready

        if condition1 or condition2:
            # âœ… æ»¡è¶³æ¡ä»¶ï¼Œè°ƒç”¨ process
            result = process(
                final_model_type,
                model_zip,
                model_config,
                data_zip,
                data_config
            )
            
            if result is True:
                st.success("âœ… æ¨¡å‹å¯¼å…¥å®Œæˆï¼")
            else:
                st.error(result)
        else:
            # âŒ ä¸æ»¡è¶³æ¡ä»¶ï¼Œæç¤ºé”™è¯¯
            missing = []
            if not model_ready:
                missing.append("æ¨¡å‹ç»„ï¼ˆéœ€åŒæ—¶ä¸Šä¼  model_zip å’Œ model_configï¼‰")
            if data_config is None:
                missing.append("data_configï¼ˆå¿…é¡»ä¸Šä¼ ï¼‰")
            if not data_ready and (data_zip is not None or data_config is not None):
                missing.append("æ•°æ®ç»„ä¸å®Œæ•´ï¼ˆéœ€åŒæ—¶ä¸Šä¼  data_zip å’Œ data_configï¼‰")

            st.error(f"""
            âš ï¸ **æäº¤å¤±è´¥ï¼**  
            è¯·ç¡®ä¿ç¬¦åˆä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€ï¼š
            - **æƒ…å†µ1**ï¼šå®Œæ•´ä¸Šä¼ æ¨¡å‹ç»„ï¼ˆ`model_zip` + `model_config`ï¼‰ï¼Œå¹¶è‡³å°‘ä¸Šä¼  `data_config`ï¼ˆ`data_zip` å¯é€‰ï¼‰ï¼Œ**æˆ–**  
            - **æƒ…å†µ2**ï¼šå®Œæ•´ä¸Šä¼ æ•°æ®ç»„ï¼ˆ`data_zip` + `data_config`ï¼‰ï¼Œä¸ä¸Šä¼ æ¨¡å‹ç»„ï¼Œ**æˆ–** 
            - **æƒ…å†µ3**: å…¨éƒ¨å®Œæ•´ä¸Šä¼  


            """)
else:
    # ----------- å½“ model_field å˜åŒ–æ—¶ï¼Œé‡ç½®æ‰€æœ‰ç›¸å…³é€‰æ‹© -----------
    def on_model_field_change():
        st.session_state["selected_models"] = []
        st.session_state["selected_datasets"] = []
        st.session_state["selected_tasks"] = []
        st.session_state["_last_selected_dataset"] = None
        
    model_field_options = get_all_model_types()  # æŒ‰ä½ çš„éœ€æ±‚å¯æ‰©å±•æˆ–è‡ªåŠ¨åŠ è½½

    # âœ… æ·»åŠ æ¨¡å‹ç‰¹å¾å­—æ®µé€‰æ‹©æ§ä»¶
    st.selectbox(
        "æ¨¡å‹è¾“å…¥ç‰¹å¾ç±»å‹ (model_field)",
        options=model_field_options,
        key="selected_model_field",
        on_change=on_model_field_change
    )    
    # ----------- ä» model_dataset_map.yaml è·å–æ•°æ®é›†åˆ—è¡¨ -----------
    @st.cache_data
    def load_model_map(modelfield, path=MODEL_PATH):
        new_path = os.path.join(path, 'models.yaml')
        with open(new_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)

        return data.get(modelfield, {}).get("models", {})

    model_field = st.session_state["selected_model_field"]
    if model_field:
        model_options =[]
        model_map = load_model_map(model_field)
        for mode_l in model_map:
            submodels = get_submodel(model_field,mode_l)
            for submodel in submodels:
                full_model = f"{mode_l}_{submodel}"
                model_options.append(full_model)
        model_options_with_all = model_options + ["all"]
        # ----------- è®°å½•æ¨¡å‹é€‰æ‹©å‰çš„å€¼ -----------


        def on_model_change():
            st.session_state["selected_datasets"] = []
            st.session_state["selected_tasks"] = []

        # âœ… å¤šé€‰æ§ä»¶ï¼ˆä½¿ç”¨ session ä¿å­˜ + å›è°ƒé‡ç½®ï¼‰
        st.multiselect(
            "æ¨¡å‹ç±»å‹ (model)",
            options=model_options_with_all,
            key="selected_models",
            on_change=on_model_change
        )

    if "all" in st.session_state["selected_models"]:
        model = model_options
    else:
        model = st.session_state["selected_models"]
        

    if model:
        model_upper_list =[]
        for models in model:
            if isinstance(models, str) and "_" in models:
                model_part = models.split("_")[0]
            else:
                model_part = str(models).upper()
            if model_part not in model_upper_list:
                model_upper_list.append(model_part)
                readname = f"{model_part}_readme.md"
                outputname = f"{model_part}_output.py"
                dataname = f"{model_part}_data.py"
                modelname = f"{model_part}_model.py"
                READMEFILE_PATH = os.path.join(project_root, 'models',model_field,readname)
                OUTPUTFILE_PATH = os.path.join(project_root, 'models',model_field,model_part,outputname)
                DATAFILE_PATH = os.path.join(project_root, 'models',model_field,model_part,dataname)
                MODELFILE_PATH=os.path.join(project_root, 'models',model_field,model_part,modelname)
                show_file_selector(f"{model_part}: README.md", READMEFILE_PATH, is_markdown=True)
                show_file_selector(f"{model_part}: Output Script", OUTPUTFILE_PATH)
                show_file_selector(f"{model_part}: Data Script", DATAFILE_PATH)
                show_file_selector(f"{model_part}: Model Script", MODELFILE_PATH)
    #--------datasets åªæœ‰åœ¨ model å‡ºç°çš„æ—¶å€™å†å‡ºç°
    def on_dataset_change():
        st.session_state["selected_tasks"] = []  # é‡ç½®ä»»åŠ¡é€‰æ‹©
        st.session_state["_last_selected_dataset"] = None  # æ¸…é™¤ä¸Šæ¬¡ä»»åŠ¡çš„ç¼“å­˜æ ‡è®°

    if "selected_datasets" not in st.session_state:
        st.session_state["selected_datasets"] = []

    if model:
        available_datasets = get_datasets_for_model(model, model_map)
        dataset_options_with_all = available_datasets + ["all"]

        st.multiselect(
            "æ•°æ®é›†åç§° (name)",
            options=dataset_options_with_all,
            key="selected_datasets",
            on_change=on_dataset_change
        )

        if "all" in st.session_state["selected_datasets"]:
            name = available_datasets
        else:
            name = st.session_state["selected_datasets"]

    # ----------- ä»»åŠ¡é€‰æ‹©ï¼ˆtarget_listï¼‰-----------
    if "selected_tasks" not in st.session_state:
        st.session_state["selected_tasks"] = []

    if "name" in locals() and name:
        if len(name) > 1:
            st.markdown("**ä»»åŠ¡åç§° (target_list):** all")
            target_list = "all"
        else:
            dataset_name = name[0]

            try:
                utils = CheckUtils(st.session_state["selected_model_field"])
                available_tasks = utils.get_datasets_measure_names(dataset_name)
                task_options_with_all = available_tasks + ["all"]

                # å¦‚æœæ¢äº†æ•°æ®é›†ï¼Œé‡ç½®ä»»åŠ¡é€‰æ‹©
                if st.session_state.get("_last_selected_dataset") != dataset_name:
                    st.session_state["selected_tasks"] = []
                    st.session_state["_last_selected_dataset"] = dataset_name

                st.multiselect(
                    "ä»»åŠ¡åç§° (target_list)",
                    options=task_options_with_all,
                    key="selected_tasks"
                )

                if "all" in st.session_state["selected_tasks"]:
                    target_list = available_tasks
                else:
                    target_list = st.session_state["selected_tasks"]

            except Exception as e:
                st.warning(f"æ— æ³•è·å–ä»»åŠ¡åˆ—è¡¨ï¼š{e}")
                target_list = "all"


    # ----------- evaluation è¾“å…¥æ¡† -----------
    if "eval" not in st.session_state:
        st.session_state["eval"] = True
    eval = st.checkbox("æ˜¯å¦è¯„ä¼°æ¨¡å‹å¹¶ç»˜å›¾ (å¿…é¡»å…ˆä¸Šä¼ æ•°æ®)", key="eval")

    # ----------- smiles_list è¾“å…¥æ¡† -----------
    if "smiles_list" not in st.session_state:
        st.session_state["smiles_list"] = "random200"
    st.markdown("### é€‰æ‹©æ•°æ®è¾“å…¥æ–¹å¼")
    mode_display_to_internal = {
        "è‡ªåŠ¨è¯„ä¼°(å¿…é¡»å…ˆä¸Šä¼ å¯¹åº”æ•°æ®)": "auto_eval",
        "ä¸Šä¼ æ–‡ä»¶": "file_upload",
        "æ‰‹åŠ¨è¾“å…¥": "manual_input"
    }
    mode_internal_to_display = {v: k for k, v in mode_display_to_internal.items()}

    # æ§ä»¶ï¼šé€‰æ‹©æ¨¡å¼ï¼ˆåªè¯»ï¼Œä¸ç›´æ¥æ”¹ session_stateï¼‰
    selected_mode_display = st.radio(
        "è¯·é€‰æ‹©ä¸€ç§æ–¹å¼",
        options=list(mode_display_to_internal.keys()),
        index=list(mode_display_to_internal.values()).index(st.session_state["smiles_input_mode"])
    )

    # å°† radio æ§ä»¶ç»“æœå†™å…¥ session
    if mode_display_to_internal[selected_mode_display] != st.session_state["smiles_input_mode"]:
        st.session_state["smiles_input_mode"] = mode_display_to_internal[selected_mode_display]
        st.rerun()

    # ä¸‰ç§æ¨¡å¼åˆ†åˆ«å¤„ç†
    mode = st.session_state["smiles_input_mode"]

    if mode == "auto_eval":
        smiles_eval_mode = st.selectbox(
            "é€‰æ‹©è¯„ä¼°æ¨¡å¼",
            ["random", "all"],
            index=["random", "all"].index(st.session_state["smiles_eval_mode"])
        )

        if smiles_eval_mode != st.session_state["smiles_eval_mode"]:
            st.session_state["smiles_eval_mode"] = smiles_eval_mode
            st.rerun()

        if st.session_state["smiles_eval_mode"] == "random":
            smiles_eval_num = st.number_input("è¯·è¾“å…¥è¦éšæœºé€‰æ‹©çš„æ•°é‡", min_value=1, value=st.session_state["smiles_eval_num"])
            if smiles_eval_num != st.session_state["smiles_eval_num"]:
                st.session_state["smiles_eval_num"] = smiles_eval_num
                st.session_state["smiles_list"] = f"random{smiles_eval_num}"
            else:
                st.session_state["smiles_list"] = f"random{st.session_state['smiles_eval_num']}"
        else:
            st.session_state["smiles_list"] = "all"

    elif mode == "file_upload":
        uploaded_file = st.file_uploader("ä¸Šä¼ åŒ…å« æ•°æ® çš„ .txt æˆ– .csv æ–‡ä»¶", type=["txt", "csv"])
        if uploaded_file is not None:
            st.session_state["smiles_file"] = uploaded_file
            if uploaded_file.name.endswith(".txt"):
                content = uploaded_file.read().decode("utf-8")
                lines = [line.strip() for line in content.splitlines() if line.strip()]
                st.session_state["smiles_list"] = lines
            elif uploaded_file.name.endswith(".csv"):
                df = pd.read_csv(uploaded_file)
                col = st.selectbox("é€‰æ‹© SMILES æ‰€åœ¨åˆ—", df.columns)
                smiles = df[col].dropna().astype(str).tolist()
                st.session_state["smiles_list"] = smiles

    elif mode == "manual_input":
        text = st.text_area("è¯·è¾“å…¥é€—å·åˆ†éš”çš„æ•°æ®", value=st.session_state["smiles_text_input"])
        if text != st.session_state["smiles_text_input"]:
            st.session_state["smiles_text_input"] = text
            smiles = [s.strip() for s in text.split(",") if s.strip()]
            st.session_state["smiles_list"] = smiles


    # ----------- è¿è¡ŒæŒ‰é’® -----------
    if st.button("è¿è¡Œæ¨¡å‹é…ç½®å¹¶ä¿å­˜é…ç½®æ–‡ä»¶"):
        fields_to_convert = ["model", "name", "target_list"]
        config = load_config()
        config["user_argument"] = st.session_state["selected_model_field"]
        config["model"] = model
        config["name"] = name
        config["target_list"] = target_list
        config["eval"] = st.session_state["eval"]
        smiles_val = st.session_state.get("smiles_list", "")
        if isinstance(smiles_val, list):
            config["smiles_list"] = ",".join(smiles_val)
        else:
            config["smiles_list"] = smiles_val
        config = list_to_csv_fields(config, fields_to_convert)

        save_config(config)
        st.success("é…ç½®å·²ä¿å­˜ï¼")

        try:
            result = subprocess.run(
                ["conda", "run", "-n", "molplat", "python", RUN_SCRIPT_PATH],
                check=True  # è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸å¦‚æœå¤±è´¥
            )
            st.success("âœ… æ¨¡å‹è¿è¡Œå®Œæˆï¼")
            result_path = os.path.join(project_root,'results','results')
            run_id,latest_run_path = get_latest_run_folder(result_path)
            history_record = {
                "timestamp": datetime.now().isoformat(),
                "run_id": run_id,
                "model_argument": config["user_argument"],
                "model": config["model"],
                "dataset": config["name"],
                "task": config["target_list"],
                "data": config["smiles_list"],
                "eval": config["eval"]
            }
            history_list = []
            if os.path.exists(HISTORY_PATH):
                with open(HISTORY_PATH, "r", encoding="utf-8") as f:
                    history_list = json.load(f)
            history_list.insert(0, history_record)
            with open(HISTORY_PATH, "w", encoding="utf-8") as f:
                json.dump(history_list, f, indent=2, ensure_ascii=False)

            if latest_run_path:
                if config['eval']:
                    plot_dir = os.path.join(latest_run_path, "plots")
                    st.markdown("## ğŸ–¼ï¸ æ¨¡å‹åˆ†æå›¾ (plots)")
                    display_images_recursively(plot_dir)

                st.markdown("## ğŸ“Š æ¨¡å‹ç»“æœè¡¨æ ¼ (CSVs)")
                display_csv_tables(latest_run_path)
            else:
                st.warning("æœªæ‰¾åˆ°ä»»ä½• runXX ç»“æœç›®å½•ã€‚")

        except subprocess.CalledProcessError:
            st.error("âŒ æ¨¡å‹è¿è¡Œå¤±è´¥ï¼")
        except Exception as e:
            st.error(f"è¿è¡Œå‡ºé”™ï¼š{e}")

    if os.path.exists(HISTORY_PATH):
        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
            history_list = json.load(f)

        if history_list:
            st.markdown("---")
            st.markdown("### ğŸ“‚ å†å²è¿è¡Œè®°å½•")
            history_labels = [f"{h['run_id']} | æ¨¡å‹: {h['model']} | æ•°æ®é›†: {h['dataset']} | ä»»åŠ¡: {h['task']}| æ•°æ®:{h['data']}" for h in history_list]
            selected_index = st.selectbox("é€‰æ‹©å†å²è®°å½•è¿è¡Œ ID ä»¥æŸ¥çœ‹ç»“æœï¼š", options=list(range(len(history_list))), format_func=lambda i: history_labels[i])

            selected = history_list[selected_index]
            selected_run_path = os.path.join(project_root, 'results', 'results', selected["run_id"])

            if os.path.exists(selected_run_path):
                if selected.get("eval", True):
                    st.markdown("## ğŸ–¼ï¸ æ¨¡å‹åˆ†æå›¾ (plots)")
                    display_images_recursively(os.path.join(selected_run_path, "plots"))

                st.markdown("## ğŸ“Š æ¨¡å‹ç»“æœè¡¨æ ¼ (CSVs)")
                display_csv_tables(selected_run_path)
            else:
                st.warning("æ‰¾ä¸åˆ°å¯¹åº”çš„å†å²ç›®å½•ã€‚")
        
            
    
